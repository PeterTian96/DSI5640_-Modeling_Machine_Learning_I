---
title: "Homework3"
author: "Zhengqi Tian"
date: "2/8/2022"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


Homework 3

Using the RMarkdown/knitr/github mechanism, implement the following tasks:
Use the prostate cancer data.
Use the cor function to reproduce the correlations listed in HTF Table 3.1, page 50.
Treat lcavol as the outcome, and use all other variables in the data set as predictors.
With the training subset of the prostate data, train a least-squares regression model with all predictors using the lm function.
Use the testing subset to compute the test error (average squared-error loss) using the fitted least-squares regression model.
Train a ridge regression model using the glmnet function, and tune the value of lambda (i.e., use guess and check to find the value of lambda that approximately minimizes the test error).
Create a figure that shows the training and test error associated with ridge regression as a function of lambda
Create a path diagram of the ridge regression analysis, similar to HTF Figure 3.8

```{r}
library('splines')        
library('tidyverse')          
library('magrittr')       
library('glmnet')         
```

# Use the prostate cancer data.
```{r}
# problem 1
prostate <- 
  read.table(url(
    'https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data'))

#subset to training examples
prostate_train <- subset(prostate, train==TRUE)%>%
  select(-train)
summary(prostate_train)


#subset to test examples
prostate_test <- subset(prostate, train==F)%>%
  select(-train)
summary(prostate_test)
```



# Use the cor function to reproduce the correlations listed in HTF Table 3.1, page 50.

```{r}
#Corrleations for train data set
cor(prostate_train)

```

# Treat lcavol as the outcome, and use all other variables in the data set as predictors.
#With the training subset of the prostate data, train a least-squares regression model with all predictors using the lm function.

```{r}
# least-squares regression
fit <- lm(lcavol ~ ., data=prostate_train)
summary(fit)
```
# Use the testing subset to compute the test error (average squared-error loss) using the fitted least-squares regression model.
```{r}
## functions to compute testing/training error w/lm
L2_loss <- function(y, yhat)
  (y-yhat)^2
error <- function(dat, fit, loss=L2_loss)
  mean(loss(dat$lcavol, predict(fit, newdata=dat)))

## train_error 
error(prostate_train, fit)

## testing error
error(prostate_test, fit)

```

# Use the testing subset to compute the test error (average squared-error loss) using the fitted least-squares regression model.
```{r}
## use glmnet to fit lasso
## glmnet fits using penalized L2 loss
## first create an input matrix and output vector
form  <- lcavol ~  lweight + age + lbph + lcp + pgg45 + lpsa + svi + gleason
x_inp <- model.matrix(form, data=prostate_train)
y_out <- prostate_train$lcavol
fit <- glmnet(x=x_inp, y=y_out, lambda=seq(0.5, 0, -0.05),alpha=0)
print(fit$beta)

## functions to compute testing/training error with glmnet
error <- function(dat, fit, lam, form, loss=L2_loss) {
  x_inp <- model.matrix(form, data=dat)
  y_out <- dat$lcavol
  y_hat <- predict(fit, newx=x_inp, s=lam)  ## see predict.elnet
  mean(loss(y_out, y_hat))
}

## train_error at lambda=0
error(prostate_train, fit, lam=0, form=form)
## testing error at lambda=0
error(prostate_test, fit, lam=0, form=form)
## train_error at lambda=0.01
error(prostate_train, fit, lam=0.01, form=form)
## testing error at lambda=0.01
error(prostate_test, fit, lam=0.01, form=form)
## train_error at lambda=0.02
error(prostate_train, fit, lam=0.02, form=form)
## testing error at lambda=0.02
error(prostate_test, fit, lam=0.02, form=form)
## train_error at lambda=0.03
error(prostate_train, fit, lam=0.03, form=form)
## testing error at lambda=0.03
error(prostate_test, fit, lam=0.03, form=form)
## train_error at lambda=0.04
error(prostate_train, fit, lam=0.04, form=form)
## testing error at lambda=0.04
error(prostate_test, fit, lam=0.04, form=form)
## train_error at lambda=0.05
error(prostate_train, fit, lam=0.05, form=form)
## testing error at lambda=0.05
error(prostate_test, fit, lam=0.05, form=form)
## train_error at lambda=0.06
error(prostate_train, fit, lam=0.06, form=form)
## testing error at lambda=0.06
error(prostate_test, fit, lam=0.06, form=form)
```

# Train a ridge regression model using the glmnet function, and tune the value of lambda (i.e., use guess and check to find the value of lambda that approximately minimizes the test error).
# Create a figure that shows the training and test error associated with ridge regression as a function of lambda
```{r}
# compute training and testing errors as function of lambda
err_train_1 <- sapply(fit$lambda, function(lam) 
  error(prostate_train, fit, lam, form))
err_test_1 <- sapply(fit$lambda, function(lam) 
  error(prostate_test, fit, lam, form))

## plot test/train error
plot(x=range(fit$lambda),
     y=range(c(err_train_1, err_test_1)),
     xlim=rev(range(fit$lambda)),
     type='n',
     xlab=expression(lambda),
     ylab='train/test error')
points(fit$lambda, err_train_1, pch=19, type='b', col='darkblue')
points(fit$lambda, err_test_1, pch=19, type='b', col='darkred')
legend('topright', c('train','test'), lty=1, pch=19,
       col=c('darkblue','darkred'), bty='n')

colnames(fit$beta) <- paste('lam =', fit$lambda)
#print(fit$beta %>% as.matrix)
```
`


# Create a path diagram of the ridge regression analysis, similar to HTF Figure 3.8

```{r}
plot(x=range(fit$lambda),
     y=range(as.matrix(fit$beta)),
     type='n',
     xlab=expression(lambda),
     ylab='Coefficients')
for(i in 1:nrow(fit$beta)) {
  points(x=fit$lambda, y=fit$beta[i,], pch=19, col='#00000055')
  lines(x=fit$lambda, y=fit$beta[i,], col='#00000055')
}
abline(h=0, lty=3, lwd=2)
#Label the lines
text(x=0, y=fit$beta[,ncol(fit$beta)], 
     labels=rownames(fit$beta))

```

