## compute and plot predictions
plot_nnet_predictions <- function(fit, dat=mixture.example) {
## create figure
plot_mixture_data()
## compute predictions from nnet
x1r <- range(dat$px1)
x2r <- range(dat$px2)
preds <- predict(fit, dat$xnew, type="class")
probs <- predict(fit, dat$xnew, type="raw")[,1]
probm <- matrix(probs, length(dat$px1), length(dat$px2))
cls <- contourLines(dat$px1, dat$px2, probm, levels=0.5)
## plot classification boundary
pls <- lapply(cls, function(p)
lines3d(p$x, p$y, z=1, col='purple', lwd=2))
## plot probability surface and decision plane
sfc <- surface3d(dat$px1, dat$px2, probs, alpha=1.0,
color="gray", specular="gray")
qds <- quads3d(x1r[c(1,2,2,1)], x2r[c(1,1,2,2)], 0.5, alpha=0.4,
color="gray", lit=FALSE)
}
plot_mixture_data(showtruth = TRUE)
plot_nnet_predictions(fit1, dat=mixture.example)
plot_mixture_data(showtruth = TRUE)
plot_nnet_predictions(fit2, dat=mixture.example)
plot_mixture_data(showtruth = TRUE)
plot_nnet_predictions(fit2, dat=mixture.example)
plot_mixture_data(showtruth = TRUE)
plot_nnet_predictions(fit2, dat=mixture.example)
plot_mixture_data(showtruth = TRUE)
plot_nnet_predictions(fit1, dat=mixture.example)
preds <- predict(fit1, dat$xnew, type="class")
probs <- predict(fit1, dat$xnew, type="raw")[,1]
View(fit1)
View(fit2)
plot_mixture_data(showtruth = TRUE)
plot_nnet_predictions(fit2, dat=mixture.example)
plot_mixture_data(showtruth = TRUE)
plot_nnet_predictions(fit1, dat=mixture.example)
View(train_x)
model %>% fit(train_x, as.matrix(train_y),epochs = 10, batch_size = 5)
plot_mixture_data(showtruth = TRUE)
plot_nnet_predictions(fit1, dat=mixture.example)
View(mixture.example)
## create 3D plot of mixture data
plot_mixture_data <- function(dat=mixture.example, showtruth=FALSE) {
## create 3D graphic, rotate to view 2D x1/x2 projection
par3d(FOV=1,userMatrix=diag(4))
plot3d(dat$xnew[,1], dat$xnew[,2], dat$prob, type="n",
xlab="x1", ylab="x2", zlab="",
axes=FALSE, box=TRUE, aspect=1)
## plot points and bounding box
x1r <- range(dat$px1)
x2r <- range(dat$px2)
pts <- plot3d(dat$x[,1], dat$x[,2], 1,
type="p", radius=0.5, add=TRUE,
col=ifelse(dat$y, "orange", "blue"))
lns <- lines3d(x1r[c(1,2,2,1,1)], x2r[c(1,1,2,2,1)], 1)
if(showtruth) {
## draw Bayes (True) classification boundary
probm <- matrix(dat$prob, length(dat$px1), length(dat$px2))
cls <- contourLines(dat$px1, dat$px2, probm, levels=0.5)
pls <- lapply(cls, function(p)
lines3d(p$x, p$y, z=1, col='purple', lwd=3))
## plot marginal probability surface and decision plane
sfc <- surface3d(dat$px1, dat$px2, dat$prob, alpha=1.0,
color="gray", specular="gray")
qds <- quads3d(x1r[c(1,2,2,1)], x2r[c(1,1,2,2)], 0.5, alpha=0.4,
color="gray", lit=FALSE)
}
}
model %>% fit(train_x, as.matrix(train_y),epochs = 10, batch_size = 5)
plot_mixture_data(showtruth = TRUE)
plot_nnet_predictions(fit1, dat=mixture.example)
fit1 <-model %>% fit(x=as.matrix(dat$x), y=as.matrix(dat$y),epochs = 10, batch_size = 5)
fit2 <-nnet(x=dat$x, y=dat$y, size = 10, entropy = TRUE, decay = 0)
plot_mixture_data(showtruth = TRUE)
plot_nnet_predictions(fit1, dat=mixture.example)
model <- keras_model_sequential()
model %>%
layer_dense(units = 10, activation = 'relu') %>%
layer_dense(units = 2, activation = 'softmax')
model %>% compile(
optimizer = 'adam',
loss = 'sparse_categorical_crossentropy',
metrics = list('accuracy')
)
model %>% fit(train_x, as.matrix(train_y),epochs = 10, batch_size = 5)
## compute and plot predictions
plot_nnet_predictions <- function(fit, dat=mixture.example) {
## create figure
plot_mixture_data()
## compute predictions from nnet
x1r <- range(dat$px1)
x2r <- range(dat$px2)
preds <- predict(fit, dat$xnew, type="class")
probs <- predict(fit, dat$xnew, type="raw")[,1]
probm <- matrix(probs, length(dat$px1), length(dat$px2))
cls <- contourLines(dat$px1, dat$px2, probm, levels=0.5)
## plot classification boundary
pls <- lapply(cls, function(p)
lines3d(p$x, p$y, z=1, col='purple', lwd=2))
## plot probability surface and decision plane
sfc <- surface3d(dat$px1, dat$px2, probs, alpha=1.0,
color="gray", specular="gray")
qds <- quads3d(x1r[c(1,2,2,1)], x2r[c(1,1,2,2)], 0.5, alpha=0.4,
color="gray", lit=FALSE)
}
fit1 <-model %>% fit(x=dat$x, y=dat$y,epochs = 10, batch_size = 5)
fit2 <-nnet(x=dat$x, y=dat$y, size = 10, entropy = TRUE, decay = 0)
plot_mixture_data(showtruth = TRUE)
plot_nnet_predictions(fit1, dat=mixture.example)
plot_mixture_data(showtruth = TRUE)
plot_nnet_predictions(fit2, dat=mixture.example)
plot_mixture_data(showtruth = TRUE)
#plot_nnet_predictions(fit2, dat=mixture.example)
plot_mixture_data(showtruth = TRUE)
#plot_nnet_predictions(fit1, dat=mixture.example)
plot_mixture_data(showtruth = TRUE)
plot_nnet_predictions(fit2, dat=mixture.example)
## plot data and 'true' probability surface
plot_mixture_data(showtruth=TRUE)
plot_mixture_data(showtruth = TRUE)
plot_nnet_predictions(fit1, dat=mixture.example)
View(mixture.example)
View(mixture.example)
View(fit1)
model <- keras_model_sequential()
model %>%
# Adds a densely-connected layer with 64 units to the model:
layer_dense(units = 64, activation = 'relu') %>%
# Add another:
layer_dense(units = 64, activation = 'relu') %>%
# Add a softmax layer with 10 output units:
layer_dense(units = 10, activation = 'softmax')
model %>% compile(
optimizer = 'adam',
loss = 'categorical_crossentropy',
metrics = list('accuracy')
)
data <- matrix(rnorm(1000 * 32), nrow = 1000, ncol = 32)
labels <- matrix(rnorm(1000 * 10), nrow = 1000, ncol = 10)
data <- matrix(rnorm(1000 * 32), nrow = 1000, ncol = 32)
labels <- matrix(rnorm(1000 * 10), nrow = 1000, ncol = 10)
model %>% fit(
data,
labels,
epochs = 10,
batch_size = 32
)
a<-model %>% fit(
data,
labels,
epochs = 10,
batch_size = 32
)
a %>% predict(data, batch_size = 32)
fit1 <-nnet(x=dat$x, y=dat$y, size = 10, entropy = TRUE, decay = 0)
plot_mixture_data(showtruth = TRUE)
plot_nnet_predictions(fit1, dat=mixture.example)
# plot keras boundary
plot_keras_preds <- function(fit, dat=mixture.example) {
## create figure
eval(plot_mixture_data)
## compute predictions from nnet
probs <- k_probs[,1]
preds <- k_preds
probm <- matrix(probs, length(dat$px1), length(dat$px2))
cls <- contourLines(dat$px1, dat$px2, probm, levels=0.5)
rslt <- sapply(cls, lines, col='black')
}
plot_keras_preds(fit1)
k_probs <- k_fit %>% predict(dat$xnew)
fit1 <- model %>% fit(train_x, as.matrix(train_y),epochs = 10, batch_size = 5)
model %>% fit(train_x, as.matrix(train_y),epochs = 10, batch_size = 5)
model <- keras_model_sequential()
model %>%
layer_dense(units = 10, activation = 'relu') %>%
layer_dense(units = 2, activation = 'softmax')
model %>% compile(
optimizer = 'adam',
loss = 'sparse_categorical_crossentropy',
metrics = list('accuracy')
)
model %>% fit(train_x, as.matrix(train_y),epochs = 10, batch_size = 5)
fit1 <- model %>% fit(train_x, as.matrix(train_y),epochs = 10, batch_size = 5)
k_probs <- fit1 %>% predict(dat$xnew)
fit2 <-nnet(x=dat$x, y=dat$y, size = 10, entropy = TRUE, decay = 0)
plot_keras_preds(fit1)
fit1 <- model %>% fit(train_x, as.matrix(train_y),epochs = 10, batch_size = 5)
k_probs <- fit1 %>% predict(dat$xnew)
fit1 <- model %>% fit(train_x, as.matrix(train_y),epochs = 10, batch_size = 5)
## compute and plot predictions
plot_nnet_predictions <- function(fit, dat=mixture.example) {
## create figure
plot_mixture_data()
## compute predictions from nnet
x1r <- range(dat$px1)
x2r <- range(dat$px2)
preds <- predict(fit, dat$xnew, type="class")
probs <- predict(fit, dat$xnew, type="raw")[,1]
probm <- matrix(probs, length(dat$px1), length(dat$px2))
cls <- contourLines(dat$px1, dat$px2, probm, levels=0.5)
## plot classification boundary
pls <- lapply(cls, function(p)
lines3d(p$x, p$y, z=1, col='purple', lwd=2))
## plot probability surface and decision plane
sfc <- surface3d(dat$px1, dat$px2, probs, alpha=1.0,
color="gray", specular="gray")
qds <- quads3d(x1r[c(1,2,2,1)], x2r[c(1,1,2,2)], 0.5, alpha=0.4,
color="gray", lit=FALSE)
}
k_probs <- fit1 %>% predict(dat$xnew)
k_probs <- fit1  %>% predict(img) %>%k_argmax()
k_probs <- fit1  %>% predict(img)
k_probs <- fit1  %>% predict(dat$xnew)
model1 %>% compile(
loss = loss_categorical_crossentropy,
optimizer = optimizer_adadelta(),
metrics = c('accuracy')
)
fit1 %>% compile(
loss = loss_categorical_crossentropy,
optimizer = optimizer_adadelta(),
metrics = c('accuracy')
)
prediction_keras <- model%>% predict(dat$x)
classes_keras <- rep(0,1)
for (i in 1:6831){
if (prediction_keras[i,1] > prediction_keras[i,2]){
classes_keras[i] <- 1
}else{
classes_keras[i] <- 2
}
}
data <-  mixture.example
train_x <- mixture.example$x
train_y <- mixture.example$y
test_x <- mixture.example$xnew
prediction_keras <- model%>% predict(dat$xnew)
classes_keras <- rep(0,1)
for (i in 1:6831){
if (prediction_keras[i,1] > prediction_keras[i,2]){
classes_keras[i] <- 1
}else{
classes_keras[i] <- 2
}
}
prediction_nnet <- fit_nnet%>% predict(dat$xnew)
fit1 <- model %>% fit(train_x, as.matrix(train_y),epochs = 10, batch_size = 5)
fit2 <- nnet(x = train_x, y= train_y, size=10, entropy=TRUE, decay=0.02)
data <-  mixture.example
train_x <- mixture.example$x
train_y <- mixture.example$y
test_x <- mixture.example$xnew
prediction_keras <- model%>% predict(dat$xnew)
classes_keras <- rep(0,1)
for (i in 1:6831){
if (prediction_keras[i,1] > prediction_keras[i,2]){
classes_keras[i] <- 1
}else{
classes_keras[i] <- 2
}
}
prediction_nnet <- fit2%>% predict(dat$xnew)
classes_nnet<- rep(0,1)
for (i in 1:6831){
if (prediction_nnet[i,1] > 0.5){
classes_nnet[i]  <- 1
}else{
classes_nnet[i]  <- 2
}
}
x <- seq(1,6831,1)
plot(x,classes_keras)
lines(x,classes_nnet,col = "lightpink",type = "p")
set.seed(999)
nn_fit <- nnet(x=mix$x, y=mix$y, size=5, entropy=TRUE, decay=0)
set.seed(999)
nn_fit <- nnet(x=dat$x, y=dat$y, size=5, entropy=TRUE, decay=0)
nn_probs <- predict(nn_fit, dat$xnew, type="raw")[,1]
nn_preds <- function() {
eval(plot_datture_data)
probs <- nn_probs
probm <- matrix(probs, length(dat$px1), length(dat$px2))
cls <- contourLines(dat$px1, dat$px2, probm, levels=0.5)
rslt <- sapply(cls, lines, col='orange', lwd = 1)
probs1 <- k_probs
probm1 <- matrix(probs1, length(dat$px1), length(dat$px2))
cls1 <- contourLines(dat$px1, dat$px2, probm1, levels=0.5)
rslt1 <- sapply(cls1, lines, col='pink', lwd = 1)
legend(-2.5,-1, legend=c("Bayes Classification Boundary",
"Keras Model Prediction",
"NNet Model Prediction"),
col=c("red", "green","yellow"), lty=1:1, cex=0.5)
}
nn_preds()
## load binary classification example data
data("mixture.example")
dat <- mixture.example
set.seed(1)
#initializes model
k_model <- keras_model_sequential()
#creates 10-node hidden layer
k_model %>%
#builds 10-node hidden layer
layer_dense(units = 10, activation = 'relu') %>%
#builds binary output layer
layer_dense(units = 2, activation = 'softmax')
#complies model
k_model %>% compile(
optimizer = 'adam',
loss = 'sparse_categorical_crossentropy',
metrics = c('accuracy')
)
## load binary classification example data
data("mixture.example")
dat <- mixture.example
set.seed(1)
#initializes model
k_model <- keras_model_sequential()
#creates 10-node hidden layer
k_model %>%
#builds 10-node hidden layer
layer_dense(units = 10, activation = 'relu') %>%
#builds binary output layer
layer_dense(units = 2, activation = 'softmax')
#complies model
k_model %>% compile(
optimizer = 'adam',
loss = 'sparse_categorical_crossentropy',
metrics = c('accuracy')
)
set.seed(1)
#fits model
k_model %>% fit(x = dat$x, y = dat$y, epochs = 5, verbose = 2)
#checks accuracy and loss
k_score <- k_model %>% evaluate(dat$x, dat$y, verbose = 0)
cat('Test loss:', k_score[1], "\n")
cat('Test accuracy:', k_score[2], "\n")
#Uses Prof. Shotwell's code to print datture data with true contour line
plot_datture_data <- expression({
plot(dat$x[,1], dat$x[,2],
col=ifelse(dat$y==0, 'blue', 'orange'),
pch=20,
xlab=expression(x[1]),
ylab=expression(x[2]))
## draw Bayes (True) classification boundary
prob <- matrix(dat$prob, length(dat$px1), length(dat$px2))
cont <- contourLines(dat$px1, dat$px2, prob, levels=0.5)
rslt <- sapply(cont, lines, col='purple')
})
#tests to ensure it works
eval(plot_datture_data)
set.seed(23188)
#Predicts values based on constructed model
k_probs = k_model %>% predict(dat$xnew)
plot_k_preds <- function() {
#plots original data
eval(plot_datture_data)
#computes and plots keras prediction
probs <- k_probs[,1]
probm <- matrix(probs, length(dat$px1), length(dat$px2))
cls <- contourLines(dat$px1, dat$px2, probm, levels=0.5)
rslt <- sapply(cls, lines, col='red', lwd = 2)
legend(-2.5,-1, legend=c("Bayes Classification Boundary", "Keras Model Prediction"), col=c("purple", "red"), lty=1:1, cex=0.5)
}
plot_k_preds()
set.seed(1)
#Predicts values based on constructed model
k_probs = k_model %>% predict(dat$xnew)
plot_k_preds <- function() {
#plots original data
eval(plot_datture_data)
#computes and plots keras prediction
probs <- k_probs[,1]
probm <- matrix(probs, length(dat$px1), length(dat$px2))
cls <- contourLines(dat$px1, dat$px2, probm, levels=0.5)
rslt <- sapply(cls, lines, col='red', lwd = 2)
legend(-2.5,-1, legend=c("Bayes Classification Boundary", "Keras Model Prediction"), col=c("purple", "red"), lty=1:1, cex=0.5)
}
plot_k_preds()
set.seed(1)
nn_fit <- nnet(x=dat$x, y=dat$y, size=5, entropy=TRUE, decay=0)
nn_probs <- predict(nn_fit, dat$xnew, type="raw")[,1]
nn_preds <- function() {
eval(plot_datture_data)
probs <- nn_probs
probm <- matrix(probs, length(dat$px1), length(dat$px2))
cls <- contourLines(dat$px1, dat$px2, probm, levels=0.5)
rslt <- sapply(cls, lines, col='orange', lwd = 1)
probs1 <- k_probs
probm1 <- matrix(probs1, length(dat$px1), length(dat$px2))
cls1 <- contourLines(dat$px1, dat$px2, probm1, levels=0.5)
rslt1 <- sapply(cls1, lines, col='pink', lwd = 1)
legend(-2.5,-1, legend=c("Bayes Classification Boundary",
"Keras Model Prediction",
"NNet Model Prediction"),
col=c("red", "green","yellow"), lty=1:1, cex=0.5)
}
nn_preds()
set.seed(1)
#Predicts values based on constructed model
k_probs = k_model %>% predict(dat$xnew)
plot_k_preds <- function() {
#plots original data
eval(plot_datture_data)
#computes and plots keras prediction
probs <- k_probs[,1]
probm <- matrix(probs, length(dat$px1), length(dat$px2))
cls <- contourLines(dat$px1, dat$px2, probm, levels=0.5)
rslt <- sapply(cls, lines, col='blue', lwd = 2)
legend(-2.5,-1, legend=c("Bayes Classification Boundary", "Keras Model Prediction"), col=c("purple", "red"), lty=1:1, cex=0.5)
}
plot_k_preds()
set.seed(1)
nn_fit <- nnet(x=dat$x, y=dat$y, size=5, entropy=TRUE, decay=0)
nn_probs <- predict(nn_fit, dat$xnew, type="raw")[,1]
nn_preds <- function() {
eval(plot_datture_data)
probs <- nn_probs
probm <- matrix(probs, length(dat$px1), length(dat$px2))
cls <- contourLines(dat$px1, dat$px2, probm, levels=0.5)
rslt <- sapply(cls, lines, col='orange', lwd = 1)
probs1 <- k_probs
probm1 <- matrix(probs1, length(dat$px1), length(dat$px2))
cls1 <- contourLines(dat$px1, dat$px2, probm1, levels=0.5)
rslt1 <- sapply(cls1, lines, col='blue', lwd = 1)
legend(-2.5,-1, legend=c("Bayes Classification Boundary",
"Keras Model Prediction",
"NNet Model Prediction"),
col=c("red", "green","yellow"), lty=1:1, cex=0.5)
}
nn_preds()
## load binary classification example data
data("datture.example")
dat <- matrix.example
## load binary classification example data
data("datture.example")
dat <- mixture.example
set.seed(1)
#initializes model
k_model <- keras_model_sequential()
#creates 10-node hidden layer
k_model %>%
#builds 10-node hidden layer
layer_dense(units = 10, activation = 'relu') %>%
#builds binary output layer
layer_dense(units = 2, activation = 'softmax')
#complies model
k_model %>% compile(
optimizer = 'adam',
loss = 'sparse_categorical_crossentropy',
metrics = c('accuracy')
)
set.seed(1)
#fits model
k_model %>% fit(x = dat$x, y = dat$y, epochs = 5, verbose = 2)
#checks accuracy and loss
k_score <- k_model %>% evaluate(dat$x, dat$y, verbose = 0)
cat('Test loss:', k_score[1], "\n")
cat('Test accuracy:', k_score[2], "\n")
#Uses Prof. Shotwell's code to print datture data with true contour line
plot_datture_data <- expression({
plot(dat$x[,1], dat$x[,2],
col=ifelse(dat$y==0, 'blue', 'orange'),
pch=20,
xlab=expression(x[1]),
ylab=expression(x[2]))
## draw Bayes (True) classification boundary
prob <- matrix(dat$prob, length(dat$px1), length(dat$px2))
cont <- contourLines(dat$px1, dat$px2, prob, levels=0.5)
rslt <- sapply(cont, lines, col='purple')
})
#tests to ensure it works
eval(plot_datture_data)
set.seed(1)
#Predicts values based on constructed model
k_probs = k_model %>% predict(dat$xnew)
plot_k_preds <- function() {
#plots original data
eval(plot_datture_data)
#computes and plots keras prediction
probs <- k_probs[,1]
probm <- matrix(probs, length(dat$px1), length(dat$px2))
cls <- contourLines(dat$px1, dat$px2, probm, levels=0.5)
rslt <- sapply(cls, lines, col='blue', lwd = 2)
legend(-2.5,-1, legend=c("Bayes Classification Boundary", "Keras Model Prediction"), col=c("purple", "red"), lty=1:1, cex=0.5)
}
plot_k_preds()
set.seed(1)
nn_fit <- nnet(x=dat$x, y=dat$y, size=5, entropy=TRUE, decay=0)
nn_probs <- predict(nn_fit, dat$xnew, type="raw")[,1]
nn_preds <- function() {
eval(plot_datture_data)
probs <- nn_probs
probm <- matrix(probs, length(dat$px1), length(dat$px2))
cls <- contourLines(dat$px1, dat$px2, probm, levels=0.5)
rslt <- sapply(cls, lines, col='orange', lwd = 1)
probs1 <- k_probs
probm1 <- matrix(probs1, length(dat$px1), length(dat$px2))
cls1 <- contourLines(dat$px1, dat$px2, probm1, levels=0.5)
rslt1 <- sapply(cls1, lines, col='blue', lwd = 1)
legend(-2.5,-1, legend=c("Bayes Classification Boundary",
"Keras Model Prediction",
"NNet Model Prediction"),
col=c("red", "green","yellow"), lty=1:1, cex=0.5)
}
nn_preds()
library(keras)
library(tensorflow)
# Please use the follow to add model
#install_keras()
